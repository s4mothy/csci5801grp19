
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ GENERAL NOTES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Current versions are online, only as long as grant runs, needs something to work with LMS, need to take problems from who knows where, and put into LMS. Needs solution that can take lots of problems and make unique quiz for each student. GIFT, UTI, Canvas, Moodle, Blackboard, systems we want. Any language should be used in these formats, but should work from source code files, generally text.
INPUT SHOULD BE TEXT.

Expanding the etc, traditional problem is scrabbling and sorting. Multiple choice is selecting individual line in source code, extracting and creating mutants. 2D parsons problem, rearranging, but now indentation. Introducing bugs to individual lines. Introducing blanks. C++/Java/Python for these, gets more complicated with Lisp, etc, needs to be applicable to those in future (use abstraction?).

Not really useful unless used by many instructors, so use many LMSs. As long as Canvas, Moodle, Blackboard, should work for 90% of instructors (needs negotiation?).

(We are implementing front end, researchers are doing backend [thank god])

Scoring is tricky, if one line out of place, kind of tricky to see how wrong it is. How many lines to move to fix? Certain versions of quizzes in can't do partial credit, others can. Grading specific to problem and LMS.

The system needs to perform mutations itself for multiple choice.

User might want to say how the mutations are done.

Once inst pulls in piece of code, they can go back and reuse again. If we can store on drive, would be nice, but having reasonable number of past problems.

Standalone application, pick file, pick type.

OS Restriction: no, just needs to work.

Should handle pseudocode (basically this shouldn't actually be running the code, by the sound of it)

Mutants for multiple choice need to NOT WORK.

Fill in the blank: Use regex to compare entered bit to correct fill.

Find the bug. Generate one mutant, swap, they need to select that line.

Can do command line, would prefer a GUI.

Don't worry about points per problem, etc. Set manually at time of quiz creation.

Minimize manual grading. Should try to handle lines with regex to avoid "Verbatum" answers failing.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ QUESTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Is there a "third user" in this example, being the student answering the parson's problems generated by the instructor/researcher/TAs?
- Is use case for instructor generating the problems up to the output files? 
- Is the TA the one supplying these files to students? Are the TAs the using the LMS?
- Will users get multiple attempts at selecting the correct code? Will they see the outcome of running their choices within the code?
- (#1) what types of source code are you looking for? complete blocks, lines? 
- Should the code be able to run regardless of the user's choices?
- Are there specific languages or ways the code should be compiled/interpreted?
- for #2, is annotating for inclusion/exclusion meaning you want that line to run if the user chooses  "include" and skip if the user chooses "exclude"?
- for line tuples (#3), are you wanting the option to create them, add lines as options? 
- Is the intent for a user to select a choice among the tuple and have that choice implemented in the code?
- For #4 can you expand on what you mean for question types? What others might appear?
- Is "matching an option" to select lines from a list and apply them in "blanks" within the code?
- Should a user be able to choose the same option multiple times? 
- If the same line is available multiple times, should they show up as an option more than once? 
- When an instructor forms the tuple, should they choose how many times each will appear as options? 
- For ordering, is the intent to have multiple lines of code where the lines order can be swapped by the user?
- For multiple choice, is there a tuple where the user's choice is then applied to that space in the code? 
- Do you want the tuple choices randomized when offered to users? Do you want them not randomized for the instructor?
- (#5) Should variations of the source code that error have clear messaging that explains where they went wrong? How clear?
- Does this need a UI for the instructors? IS it just a file they run from console with inputs? 
- What should the user interactions with the instructors look like? Is the LMS outlined in #6 and #7 referring to a UI for the user?
- Is the output file suggested in #7 referring to the instructors using this system to create a file for the users that includes the questions in the source code and once chosen by the user would create the appropriate output for the source code, all of which would be included in the LMS for the user?
- For #8, should the output files be editable? Should the instructors be able to save templates they can reopen before exporting to an output file?

## STUFF DIRECTLY ANSWERED ## 
- For the researcher user base, is the use case for the system similar to instructors or more based on outputs and statistics outside grading?
ANS:
What types were created, how instructors use it. Good data. Nice to have usage data, track variations, usage statistics help researchers figure out what sort of choices instructors are making, and figure out why they are doing that.

- For an individual source code, how many questions are being asked for it? Would one source code include a matching and ordering question?
ANS:
User is okay with one file, one type. No need for multiple types.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
